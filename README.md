<p align="center">

  <h2 align="center"><strong>CoRe: Learning Compact Representation with Prior-Guided Initialization</strong></h2>

<div align="center">
<h5>
<em>Yu Song<sup>1</sup>, Junshan Xie<sup>1 </sup>, Jing Zhang<sup>2</sup>, Lili Yang<sup>3</sup>, Shihua Zhang<sup>4</sup>, Deyu Meng<sup>5</sup>,<br/> Xiaohui Yang<sup>1</sup> </em>
    <br><br>
       	<sup>1</sup> Henan University, China, <sup>2</sup> Wuhan University, China, <br/> <sup>3</sup> Southern University of Science and Technology, China, <sup>4</sup> Academy of Mathematics and Systems Science, Chinese Academy of Sciences, China <sup>5</sup> Xiâ€™an Jiaotong University, China
    </h5>
</div>

# ðŸ“š Contents

- [Abstract](#abstract)

# ðŸ“„Abstract
We introduce the CoRe moduleâ€”a practical compact representation module that bridges the gap between the theoretical foundations of sparse coding and empirical deep learning practices. By incorporating a prior-guided initialization strategy, we propose CoRe method which provides a new explanatory perspective on residual structures. Through rigorous mathematical analysis, we demonstrate that CoRe method reduces information loss more effectively than traditional CSC method and exhibits structural advantages. We further develop a plug-and-play CoRe module by unfolding the iterative optimization into a learnable component, which can be seamlessly integrated into both CNN and Transformer architectures via convolutional layer replacement or direct insertion. Experiments across a wide range of modelsâ€”including ResNet, YOLO, U-Net, MobileNet, EfficientViT, and SwinIRâ€”confirm the versatility of CoRe module. Comprehensive evaluations show consistent performance improvements across classification, detection, segmentation, and image denoising tasks.
